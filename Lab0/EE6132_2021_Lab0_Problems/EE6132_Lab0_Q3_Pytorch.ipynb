{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"EE6132_Lab0_Q3_Pytorch.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPVQkXM9XgZcAWDE+4tKZYW"},"kernelspec":{"name":"python3","display_name":"Python 3.9.6 64-bit ('env-01': conda)"},"language_info":{"name":"python","version":"3.9.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"8bcf250f80d2e9bc2e64296a8e3f43f7e6c00983a87ae0f6e4e2b2ba48d25fa8"}},"cells":[{"cell_type":"code","execution_count":1,"source":["import numpy as np\r\n","print(np.__version__)\r\n","import torch\r\n","print(torch.__version__)\r\n","import torch.nn.functional as F\r\n","import matplotlib.pyplot as plt\r\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["1.20.3\n","1.8.1\n"]}],"metadata":{"id":"alrXoCflGKM8"}},{"cell_type":"code","execution_count":11,"source":["import sys"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["if torch.cuda.is_available():  \r\n","  dev = \"cuda:0\" \r\n","else:  \r\n","  dev = \"cpu\" \r\n","\r\n","device = torch.device(dev)\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["print('__Python VERSION:', sys.version)\r\n","print('__pyTorch VERSION:', torch.__version__)\r\n","print('__CUDA VERSION', )\r\n","from subprocess import call\r\n","# call([\"nvcc\", \"--version\"]) does not work\r\n","! nvcc --version\r\n","print('__CUDNN VERSION:', torch.backends.cudnn.version())\r\n","print('__Number CUDA Devices:', torch.cuda.device_count())\r\n","print('__Devices')\r\n","# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\r\n","print('Active CUDA Device: GPU', torch.cuda.current_device())\r\n","print ('Available devices ', torch.cuda.device_count())\r\n","print ('Current cuda device ', torch.cuda.current_device())"],"outputs":[{"output_type":"stream","name":"stdout","text":["__Python VERSION: 3.9.6 (default, Jul 30 2021, 11:42:22) [MSC v.1916 64 bit (AMD64)]\n","__pyTorch VERSION: 1.8.1\n","__CUDA VERSION\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2019 NVIDIA Corporation\n","Built on Wed_Oct_23_19:32:27_Pacific_Daylight_Time_2019\n","Cuda compilation tools, release 10.2, V10.2.89\n","__CUDNN VERSION: 7604\n","__Number CUDA Devices: 1\n","__Devices\n","Active CUDA Device: GPU 0\n","Available devices  1\n","Current cuda device  0\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# Pytorch Basics:\n"],"metadata":{"id":"kkoWEQE4jf1V"}},{"cell_type":"markdown","source":["**Exercise #1:** Torch Tensors\n","\n","**Your Task**: Convert the list to a tensor and check its properties.\n","\n","*   the following list of integers <i>[0, 1, 2, 3, 4]</i> to a torch *int* tensor.\n","*   the following float list <i>[0.0, 1.0, 2.0, 3.0, 4.0]</i> to a torch *float* tensor.\n","*   Convert the torch int tensor to torch float tensor.\n","\n"],"metadata":{"id":"MiTN0fMclcwt"}},{"cell_type":"code","execution_count":null,"source":["# Convert a integer list with length 5 to a tensor\r\n","int_list = [1,2,3,4,5]\r\n","# YOUR CODE STARTS HERE\r\n","ints_to_tensor = \r\n","#YOUR CODE ENDS HERE\r\n","print(\"The dtype of tensor object after converting it to tensor: \", ints_to_tensor.dtype)\r\n","print(\"The type of tensor object after converting it to tensor: \", ints_to_tensor.type())\r\n","print(\"The size of the ints_to_tensor: \", ints_to_tensor.size())\r\n","print(\"The dimension of the ints_to_tensor: \",ints_to_tensor.ndimension())\r\n"],"outputs":[],"metadata":{"id":"qIupvreRjfFL"}},{"cell_type":"code","execution_count":null,"source":["# Convert a float list with length 5 to a tensor\r\n","float_list = [0.0, 1.0, 2.0, 3.0, 4.0]\r\n","# YOUR CODE STARTS HERE\r\n","floats_to_tensor = \r\n","#YOUR CODE ENDS HERE\r\n","\r\n","print(\"The dtype of tensor object after converting it to tensor: \", floats_to_tensor.dtype)\r\n","print(\"The type of tensor object after converting it to tensor: \", floats_to_tensor.type())\r\n","print(\"The size of the floats_to_tensor: \", floats_to_tensor.size())\r\n","print(\"The dimension of the floats_to_tensor: \",floats_to_tensor.ndimension())"],"outputs":[],"metadata":{"id":"cjxqGezqL2gC"}},{"cell_type":"code","execution_count":null,"source":["# Convert the integer list to float tensor\r\n","old_int_tensor = torch.tensor([0, 1, 2, 3, 4])\r\n","# YOUR CODE STARTS HERE\r\n","new_float_tensor = \r\n","#YOUR CODE ENDS HERE\r\n","\r\n","print(\"The type of the new_float_tensor:\", new_float_tensor.type())\r\n","print(\"The size of the new_float_tensor: \", new_float_tensor.size())\r\n","print(\"The dimension of the new_float_tensor: \",new_float_tensor.ndimension())"],"outputs":[],"metadata":{"id":"j7g2AIN4MioI"}},{"cell_type":"markdown","source":["**Exercise #2:** 2D Torch Tensors and 2D numpy arrays\n","\n","**Your Task**: numpy vs. torch\n","\n","*   Convert the given numpy array to a torch tensor; And torch tensor to a numpy array\n"],"metadata":{"id":"moNAyO_qN3oe"}},{"cell_type":"code","execution_count":null,"source":["twoD_list = [[11, 12, 13], [21, 22, 23], [31, 32, 33]]\r\n","twoD_numpy = np.asarray(twoD_list)\r\n","print(\"The numpy array: \", twoD_numpy)\r\n","print(\"Type : \", twoD_numpy.dtype)\r\n","\r\n","# Convert numpy array to tensor\r\n","# YOUR CODE STARTS HERE\r\n","twoD_tensor = \r\n","#YOUR CODE ENDS HERE\r\n","print(\"\\nNumpy Array -> Tensor:\")\r\n","print(\"The tensor after converting:\", twoD_tensor)\r\n","print(\"Type after converting: \", twoD_tensor.dtype)\r\n","\r\n","# Convert torch tensor to numpy array\r\n","# YOUR CODE STARTS HERE\r\n","new_twoD_numpy = \r\n","#YOUR CODE ENDS HERE\r\n","print(\"\\nTensor -> Numpy Array:\")\r\n","print(\"The numpy array after converting: \", new_twoD_numpy)\r\n","print(\"Type after converting: \", new_twoD_numpy.dtype)\r\n"],"outputs":[],"metadata":{"id":"JV9SfnYAN2y_"}},{"cell_type":"markdown","source":["**Exercise #3:** 2D Torch Tensors and 2D numpy arrays\n","\n","**Your Task**: Access the different elements of the tensor `twoD_tensor` and numpyarray `twoD_numpy`.\n","\n"],"metadata":{"id":"7ATEG19PPqsn"}},{"cell_type":"code","execution_count":null,"source":["# Slice rows 2nd and 3rd row\r\n","# YOUR CODE STARTS HERE\r\n","sliced_tensor = \r\n","sliced_numpy = \r\n","# YOUR CODE STARTS HERE\r\n","\r\n","print(\"Tensor: Result after tensor slicing \", sliced_tensor)\r\n","print(\"Tensor: Dimension after tensor slicing \", sliced_tensor.ndimension())\r\n","print(\"Numpy: Result after np slicing: \", sliced_numpy)\r\n","print(\"Numpy: Dimension after np slicing: \", sliced_numpy.ndim)"],"outputs":[],"metadata":{"id":"oWuHGIbnPrjX"}},{"cell_type":"markdown","source":["<h2 id=\"Tensor_Op\">Tensor Operations</h2> "],"metadata":{"id":"O52E8XJJRxtc"}},{"cell_type":"markdown","source":["**Exercise #4:** Dot Product\n","\n","In this task, you will implement the dot product function for numpy arrays & torch tensors.\n","\n","The dot product (also known as the scalar product or inner product) is the linear combination of the n real components of two vectors.\n","\n","$$x \\cdot y = x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n$$\n","\n","**Your Task**: Implement the functions `NUMPY_dot` & `PYTORCH_dot`."],"metadata":{"id":"_1suut6QGZvp"}},{"cell_type":"code","execution_count":null,"source":["def NUMPY_dot(x, y):\r\n","    \"\"\"\r\n","    Dot product of two arrays.\r\n","\r\n","    Parameters: \r\n","    x (numpy.ndarray): 1-dimensional numpy array.\r\n","    y (numpy.ndarray): 1-dimensional numpy array.\r\n","\r\n","    Returns: \r\n","    numpy.int64: scalar quantity.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"6rNTB943Gjgw"}},{"cell_type":"code","execution_count":null,"source":["def PYTORCH_dot(x, y):\r\n","    \"\"\"\r\n","    Dot product of two tensors.\r\n","\r\n","    Parameters: \r\n","    x (torch.Tensor): 1-dimensional torch tensor.\r\n","    y (torch.Tensor): 1-dimensional torch tensor.\r\n","\r\n","    Returns: \r\n","    torch.int64: scalar quantity.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"MdpNs2pAGlEF"}},{"cell_type":"code","execution_count":null,"source":["# TEST cases\r\n","X = np.asarray([1,2,3])\r\n","Y = np.asarray([4,-5,6])\r\n","print(f'NUMPY: Dot product of {X} and {Y} is {NUMPY_dot(X,Y)}')\r\n","assert NUMPY_dot(X,Y)==12\r\n","\r\n","X = torch.from_numpy(X)\r\n","Y = torch.from_numpy(Y)\r\n","print(f'Pytorch: Dot product of {X} and {Y} is {PYTORCH_dot(X,Y)}')\r\n","assert PYTORCH_dot(X,Y).item()==12"],"outputs":[],"metadata":{"id":"VStlPUWhUTnd"}},{"cell_type":"markdown","source":["**Exercise #5:** Outer Product\n","\n","In this task, you will implement the outer product function for numpy arrays & torch tensors.\n","\n","The outer product (also known as the tensor product) of vectors x and y is defined as\n","\n","$$\n","x \\otimes y =\n","\\begin{bmatrix}\n","x_1 y_1 & x_1 y_2 & … & x_1 y_n\\\\\n","x_2 y_1 & x_2 y_2 & … & x_2 y_n\\\\\n","⋮ & ⋮ & ⋱ & ⋮ \\\\\n","x_m y_1 & x_m y_2 & … & x_m y_n\n","\\end{bmatrix}\n","$$\n","\n","**Your Task**: Implement the functions `NUMPY_outer` & `PYTORCH_outer`."],"metadata":{"id":"21DOFINXGNCf"}},{"cell_type":"code","execution_count":null,"source":["def NUMPY_outer(x, y):\r\n","    \"\"\"\r\n","    Compute the outer product of two vectors.\r\n","\r\n","    Parameters: \r\n","    x (numpy.ndarray): 1-dimensional numpy array.\r\n","    y (numpy.ndarray): 1-dimensional numpy array.\r\n","\r\n","    Returns: \r\n","    numpy.ndarray: 2-dimensional numpy array.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"2Hm0jLIDGOJX"}},{"cell_type":"code","execution_count":null,"source":["def PYTORCH_outer(x, y):\r\n","    \"\"\"\r\n","    Compute the outer product of two vectors.\r\n","\r\n","    Parameters: \r\n","    x (torch.Tensor): 1-dimensional torch tensor.\r\n","    y (torch.Tensor): 1-dimensional torch tensor.\r\n","\r\n","    Returns: \r\n","    torch.Tensor: 2-dimensional torch tensor.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"DEFE0kPkVR3d"}},{"cell_type":"code","execution_count":null,"source":["# TEST cases\r\n","X = np.asarray([1,2])\r\n","Y = np.asarray([1,3])\r\n","print(f'NUMPY: Dot product of {X} and {Y} is \\n{NUMPY_outer(X,Y)}')\r\n","\r\n","x = np.array(['a', 'b', 'c'], dtype=object)\r\n","print(f'NUMPY: Dot product of {X} and {Y} is \\n{NUMPY_outer(x, [1, 2, 3])}')\r\n","\r\n","X = torch.from_numpy(X)\r\n","Y = torch.from_numpy(Y)\r\n","print(f'Pytorch: Dot product of {X} and {Y} is \\n{PYTORCH_outer(X,Y)}')"],"outputs":[],"metadata":{"id":"45jAXfICVTE3"}},{"cell_type":"markdown","source":["**Exercise #6:** Hadamard Product\n","\n","In this task, you will implement the Hadamard product function, `multiply`, for numpy arrays & torch tensors.\n","\n","The Hadamard product (also known as the Schur product or entrywise product) of vectors x and y is defined as\n","\n","$$\n","x \\circ y =\n","\\begin{bmatrix}\n","x_{1} y_{1} & x_{2} y_{2} & … & x_{n} y_{n}\n","\\end{bmatrix}\n","$$\n","\n","**Your Task**: Implement the functions `NUMPY_multiply` & `PYTORCH_multiply`."],"metadata":{"id":"SQotAIuzYiqM"}},{"cell_type":"code","execution_count":null,"source":["def NUMPY_multiply(x, y):\r\n","    \"\"\"\r\n","    Multiply arguments element-wise.\r\n","\r\n","    Parameters: \r\n","    x (numpy.ndarray): 1-dimensional numpy array.\r\n","    y (numpy.ndarray): 1-dimensional numpy array.\r\n","\r\n","    Returns: \r\n","    numpy.ndarray: 1-dimensional numpy array.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"y9_9W6BCYjmr"}},{"cell_type":"code","execution_count":null,"source":["def PYTORCH_multiply(x, y):\r\n","    \"\"\"\r\n","    Multiply arguments element-wise.\r\n","\r\n","    Parameters: \r\n","    x (torch.Tensor): 1-dimensional torch tensor.\r\n","    y (torch.Tensor): 1-dimensional torch tensor.\r\n","\r\n","    Returns: \r\n","    torch.Tensor: 1-dimensional torch tensor.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"H3-91qalYplz"}},{"cell_type":"code","execution_count":null,"source":["# TEST cases\r\n","X = np.asarray([1,2,3])\r\n","Y = np.asarray([4,-5,6])\r\n","print(f'NUMPY: Dot product of {X} and {Y} is {NUMPY_multiply(X,Y)}')\r\n","\r\n","X = torch.from_numpy(X)\r\n","Y = torch.from_numpy(Y)\r\n","print(f'Pytorch: Dot product of {X} and {Y} is {PYTORCH_multiply(X,Y)}')"],"outputs":[],"metadata":{"id":"k3PzQ-9CYq5j"}},{"cell_type":"markdown","source":["**Exercise #7:** ReLU\n","\n","In this task, you will implement the ReLU activation function for numpy arrays and torch tensors.\n","\n","The ReLU activation (also known as the rectifier or rectified linear unit) matrix Z resulting from applying the ReLU function to matrix X is defined such that for $X,Z \\in M_{m \\times n} (\\mathbb{R})$, \n","\n","$$Z = {\\tt ReLU}(X) \\implies \\begin{cases}z_{ij} = x_{ij}&{\\mbox{if }}x_{ij}>0\\\\z_{ij} = 0&{\\mbox{otherwise.}}\\end{cases}$$\n","\n","For reference, it is common to use the notation $X = (x_{ij})$ and $Z = (z_{ij})$.\n","\n","**Your Task:** Implement the functions `NUMPY_ReLU` & `PYTORCH_ReLU`."],"metadata":{"id":"HpInjaNXaA12"}},{"cell_type":"code","execution_count":null,"source":["def NUMPY_ReLU(x):\r\n","    \"\"\"\r\n","    Applies the rectified linear unit function element-wise.\r\n","\r\n","    Parameters: \r\n","    x (numpy.ndarray): 2-dimensional numpy array.\r\n","\r\n","    Returns: \r\n","    numpy.ndarray: 2-dimensional numpy array.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"IYx9zXS9ZbNS"}},{"cell_type":"code","execution_count":null,"source":["def PYTORCH_ReLU(x):\r\n","    \"\"\"\r\n","    Applies the rectified linear unit function element-wise.\r\n","\r\n","    Parameters: \r\n","    x (torch.Tensor): 2-dimensional torch tensor.\r\n","\r\n","    Returns: \r\n","    torch.Tensor: 2-dimensional torch tensor.\r\n","    \"\"\"\r\n","    # YOUR CODE STARTS HERE\r\n","    \r\n","    # YOUR CODE ends HERE\r\n","\r\n","    return None"],"outputs":[],"metadata":{"id":"IQL_PfxoaDoR"}},{"cell_type":"code","execution_count":null,"source":["#Test Cases\r\n","np.random.seed(0)\r\n","X = np.random.randint(-1000, 1000, size=(3,3))\r\n","print(NUMPY_ReLU(X))\r\n","\r\n","X = torch.from_numpy(X)\r\n","print(PYTORCH_ReLU(X))"],"outputs":[],"metadata":{"id":"xQd8n401aE2q"}},{"cell_type":"markdown","source":["# Pytorch Differentiation"],"metadata":{"id":"reRDSEkyTtuU"}},{"cell_type":"markdown","source":["**Exercise #8:** \n","\n","**Your Task**: Determine the derivative of $ y = 2x^3+x $ at $x=1$"],"metadata":{"id":"mNgOZUQaT_ny"}},{"cell_type":"code","execution_count":null,"source":["# YOUR CODE STARTS HERE\r\n","x = \r\n","y = \r\n","\r\n","# YOUR CODE ends HERE\r\n","print('the graid of y wrt x at x = 1 is ',x.grad)\r\n","assert x.grad.item()==7"],"outputs":[],"metadata":{"id":"O2royvcJb35_"}},{"cell_type":"markdown","source":["**Exercise #9:** \n","\n","**Your Task**: Determine the <b>partial derivative</b> of the function: $f(u,v)=vu+u^{2}$ at $(u,v)=(1,2)$\n","\n","*   with respect to u\n","*   with respect to v\n","\n"],"metadata":{"id":"SnM5TjYAUsv_"}},{"cell_type":"code","execution_count":null,"source":["# YOUR CODE STARTS HERE\r\n","\r\n","# Calculate f(u, v) = v * u + u^2 at u = 1, v = 2\r\n","u = \r\n","v = \r\n","f = \r\n","print(\"The result of v * u + u^2: \", f)\r\n","\r\n","# Calculate the derivative with respect to u\r\n","\r\n","print(\"The partial derivative with respect to u: \", u.grad)\r\n","\r\n","# Calculate the derivative with respect to v\r\n","print(\"The partial derivative with respect to u: \", v.grad)\r\n","# YOUR CODE ENDS HERE\r\n"],"outputs":[],"metadata":{"id":"b5lh_HITUXZd"}},{"cell_type":"markdown","source":["**Exercise #10:** \n","\n","**Your Task**: Visualize the relu activation function and it derivative\n","\n"],"metadata":{"id":"4idmC9bqWB-S"}},{"cell_type":"code","execution_count":null,"source":["x = torch.linspace(-3, 3, 100, requires_grad = True)\r\n","# Take the derivative of Relu with respect to multiple value in x.\r\n","Y = \r\n","\r\n","# Plot out the function and its derivative\r\n","plt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'ReLU function')\r\n","plt.plot(x.detach().numpy(), x.grad.numpy(), label = 'ReLU derivative')\r\n","plt.xlabel('x')\r\n","plt.legend()\r\n","plt.show()"],"outputs":[],"metadata":{"id":"o1ldzexvVTL_"}}]}