{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azYI9OFS-m_l"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb --upgrade\n",
        "# !pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR6aBoqLTUXV",
        "outputId": "a3131767-1173-41e3-c76e-5072682b0075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "bpAn9v9eCVNG"
      },
      "outputs": [],
      "source": [
        "# Importing relevant libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "# import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "# from keras.datasets import mnist\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "i-oEGwNOWUci"
      },
      "outputs": [],
      "source": [
        "# from torchvision import datasets\n",
        "# from torchvision.transforms import ToTensor\n",
        "# from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "j_v657dT1v0G"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GeForce 920M\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        }
      ],
      "source": [
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "rRL-aJV_9WNj"
      },
      "outputs": [],
      "source": [
        "# Directories to store the models and plots\n",
        "direc_main = './drive/MyDrive/Semester_7/Advanced_Signal_Processing/A4_RNN/'\n",
        "direc_pics = './drive/MyDrive/Semester_7/Advanced_Signal_Processing/A4_RNN/pics/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2-XDxmZV7lk"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XrTMmhaV_mr"
      },
      "source": [
        "## From Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "lMBfRylhAYCa"
      },
      "outputs": [],
      "source": [
        "def loadMNIST(return_images=False, is_val_split=False, val_samples=10000, seed_value=1):\n",
        "    '''\n",
        "    ################\n",
        "    Function which returns a dictionary containing the shuffled version of the MNIST dataset.\n",
        "    Arguments:\n",
        "    return_images   [bool]  Whether we need to return images            (default: False)\n",
        "                                True: returns images\n",
        "                                False: returns flattened vectors\n",
        "    is_val_split    [bool]  Whether validation split needs to be done   (default: False)\n",
        "                                True: returns train(60K-val_samples), validation(val_samples) and test(10K)\n",
        "                                False: returns train(60K) and test(10K)\n",
        "    val_samples     [int]   Number of validation samples to take out of the training set of 60K samples     (default: 10K)\n",
        "    seed            [int]   Seed value for the numpy random shuffling   (default: 1)\n",
        "    ########\n",
        "    Return:\n",
        "    Dictionary containing the numpy arrays corresponding to train, test and val(if is_val_split==True)\n",
        "                    dict:   train:  X\n",
        "                                    y\n",
        "                            test:   X\n",
        "                                    y\n",
        "                            val:    X           (if is_val_split == True)\n",
        "                                    y           (if is_val_split == True)\n",
        "    ################\n",
        "    '''\n",
        "\n",
        "    # Loading MNIST data onto numpy arrays\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Shuffling data\n",
        "    np.random.seed(seed_value)\n",
        "    train_shuffler = np.random.permutation(60000)\n",
        "    x_train, y_train = x_train[train_shuffler], y_train[train_shuffler]\n",
        "    test_shuffler = np.random.permutation(10000)\n",
        "    x_test, y_test = x_test[test_shuffler], y_test[test_shuffler]\n",
        "\n",
        "    # Normalizing the input data\n",
        "    x_train = np.array(x_train/255.0, dtype='float32')\n",
        "    x_test = np.array(x_test/255.0, dtype='float32')\n",
        "\n",
        "    ## Splitting data appropriately\n",
        "    # Number of training samples \n",
        "    train_samples = 60000 - val_samples\n",
        "\n",
        "    if is_val_split == True:\n",
        "        # Splitting the train set into the new train and val sets\n",
        "        x_train, x_val = x_train[:train_samples], x_train[train_samples:]\n",
        "        y_train, y_val = y_train[:train_samples], y_train[train_samples:]\n",
        "\n",
        "        # Flattening the 28x28 images into vectors and then returning\n",
        "        if (return_images==False):\n",
        "            return {\n",
        "                'train':{\n",
        "                    'X': x_train.reshape([train_samples, 784]),\n",
        "                    'Y': y_train.reshape([train_samples])\n",
        "                },\n",
        "                'val':{\n",
        "                    'X': x_val.reshape([val_samples, 784]),\n",
        "                    'Y': y_val.reshape([val_samples])\n",
        "                },\n",
        "                'test':{\n",
        "                    'X': x_test.reshape([10000, 784]),\n",
        "                    'Y': y_test.reshape([10000])\n",
        "                }\n",
        "            }\n",
        "        # Returning the 28x28 images as is\n",
        "        else :\n",
        "            return {\n",
        "                'train':{\n",
        "                    'X': x_train,\n",
        "                    'Y': y_train\n",
        "                },\n",
        "                'val':{\n",
        "                    'X': x_val,\n",
        "                    'Y': y_val\n",
        "                },\n",
        "                'test':{\n",
        "                    'X': x_test,\n",
        "                    'Y': y_test\n",
        "                }\n",
        "            }\n",
        "    # Training set is not split\n",
        "    else:\n",
        "        # Flattening the images and then returning\n",
        "        if (return_images==False):\n",
        "            return {\n",
        "                'train':{\n",
        "                    'X': x_train.reshape([60000, 784]),\n",
        "                    'Y': y_train.reshape([60000])\n",
        "                },\n",
        "                'test':{\n",
        "                    'X': x_test.reshape([10000, 784]),\n",
        "                    'Y': y_test.reshape([10000])\n",
        "                }\n",
        "            }\n",
        "        # Returning the images as is\n",
        "        else :\n",
        "            return {\n",
        "                'train':{\n",
        "                    'X': x_train,\n",
        "                    'Y': y_train\n",
        "                },\n",
        "                'test':{\n",
        "                    'X': x_test,\n",
        "                    'Y': y_test\n",
        "                }\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "NmIvE1tIB0QQ"
      },
      "outputs": [],
      "source": [
        "def makeOneHot(data):\n",
        "    \"\"\"\n",
        "    Function to make the numeric labels into one-hot representation.\n",
        "    Arguments:\n",
        "    data            Dictionary as generated by loadMNIST()\n",
        "\n",
        "    Returns:\n",
        "    Similar dictionary as data but with one-hot arrays for the labels\n",
        "    \"\"\"\n",
        "    # Initializing the dict\n",
        "    data_mod = {}\n",
        "    for key in data:\n",
        "        data_mod[key] = {}\n",
        "        data_mod[key]['X'] = data[key]['X']\n",
        "        y = np.zeros((data[key]['Y'].shape[0], 10))\n",
        "        y[range(data[key]['Y'].shape[0]), data[key]['Y']] = 1\n",
        "        data_mod[key]['Y'] = y\n",
        "\n",
        "    return data_mod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "d8jH7NNXdiN2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['train', 'val', 'test'])\n",
            "(28,)\n",
            "(50000, 10)\n",
            "(10000, 28, 28)\n",
            "(10000, 10)\n",
            "(10000, 28, 28)\n",
            "(10000, 10)\n",
            "(10000,)\n",
            "(10000,)\n",
            "(10000,)\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "[2 0 6 1 9 0 2 8]\n"
          ]
        }
      ],
      "source": [
        "# Loading data as flattened vectors with a validation set\n",
        "val_samples = 10000\n",
        "data = loadMNIST(return_images=True, is_val_split=True, val_samples=val_samples, seed_value=1)\n",
        "data_oh = makeOneHot(data)\n",
        "\n",
        "print(data_oh.keys())\n",
        "print(data_oh['train']['X'][0,0].shape)\n",
        "print(data_oh['train']['Y'].shape)\n",
        "print(data_oh['test']['X'].shape)\n",
        "print(data_oh['test']['Y'].shape)\n",
        "print(data_oh['val']['X'].shape)\n",
        "print(data_oh['val']['Y'].shape)\n",
        "\n",
        "print(data['test']['Y'].shape)\n",
        "print(data['val']['Y'].shape)\n",
        "print(data['val']['Y'].shape)\n",
        "print(data_oh['val']['Y'][:8])\n",
        "print(data['val']['Y'][:8])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "6YxHvy1BkljP"
      },
      "outputs": [],
      "source": [
        "# Initializing pytorch dataset from the numpy datasets we have so far\n",
        "data_torch = {\n",
        "    'train': {\n",
        "        'X': torch.from_numpy(data['train']['X']),\n",
        "        'Y': torch.from_numpy(data['train']['Y'].astype(np.int64))\n",
        "    },\n",
        "    'test': {\n",
        "        'X': torch.from_numpy(data['test']['X']),\n",
        "        'Y': torch.from_numpy(data['test']['Y'].astype(np.int64))\n",
        "    },\n",
        "    'val':{\n",
        "        'X': torch.from_numpy(data['val']['X']),\n",
        "        'Y': torch.from_numpy(data['val']['Y'].astype(np.int64))\n",
        "    }\n",
        "}\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(data_torch['train']['X'], data_torch['train']['Y'])\n",
        "test_data = torch.utils.data.TensorDataset(data_torch['test']['X'], data_torch['test']['Y'])\n",
        "val_data = torch.utils.data.TensorDataset(data_torch['val']['X'], data_torch['val']['Y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "Whm33NV5bU4Z",
        "outputId": "e662d640-bbb4-4a94-f69c-757a12773896"
      },
      "outputs": [],
      "source": [
        "# Plotting 25 random data points from the dataset to get an idea of the dataset\n",
        "prefix = direc_pics + 'dset_'\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 5, 5\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
        "    img, label = train_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title('Number {}'.format(label), fontsize=9)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(torch.reshape(img, (28,28)), cmap=\"gray\")\n",
        "plt.savefig(prefix + '25point.png',bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "iWLu5HgnD-WR"
      },
      "outputs": [],
      "source": [
        "# Train has 600 batches of size 100. Test and Val are just 1 batch.\n",
        "loaders = {\n",
        "    'train' : torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=100, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=1),\n",
        "    \n",
        "    'test'  : torch.utils.data.DataLoader(test_data, \n",
        "                                          batch_size=10000, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=1),\n",
        "\n",
        "    'val'   : torch.utils.data.DataLoader(val_data, \n",
        "                                          batch_size=val_samples, \n",
        "                                          shuffle=True, \n",
        "                                          num_workers=1),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeWCaMWmtvxy"
      },
      "source": [
        "# Training and Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "UYpD1p2Ot7tQ"
      },
      "outputs": [],
      "source": [
        "def train_loop(loaders, model, loss_fn, optimizer, interval=75):\n",
        "    '''\n",
        "    Function to train the model and log required information\n",
        "    Arguments:\n",
        "    loaders                 dict containing DataLoader objects for the data\n",
        "    model                   The neural network we want to train\n",
        "    loss_fn                 The loss function we are trying to minimize\n",
        "    optimizer               Optimizer that we will use\n",
        "    interval                Interval between logging of loss & calculating test metrics [default: 75]\n",
        "    Returns:    Dict containing lists of training losses and test losses.\n",
        "    '''\n",
        "    dataloader = loaders['train']\n",
        "    size = len(dataloader.dataset)\n",
        "    losses = []\n",
        "    losses_test = []\n",
        "    acc_test = []\n",
        "\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        temp = model(X)\n",
        "        loss = loss_fn(input=temp['out'], target=y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % interval == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            temp1 = test_loop(loaders, model, loss_fn, option='val')\n",
        "            losses_test.append(temp1['loss'])\n",
        "            acc_test.append(temp1['acc'])\n",
        "        \n",
        "    return {\n",
        "        'losses': losses,\n",
        "        'losses_test': losses_test,\n",
        "        'acc_test': acc_test\n",
        "    }\n",
        "\n",
        "\n",
        "def test_loop(loaders, model, loss_fn, option='val'):\n",
        "    '''\n",
        "    Function to calculate loss and accuracy of the model on the test set.\n",
        "    Arguments:\n",
        "    loaders                 dict containing DataLoader objects for the data\n",
        "    model                   The neural network we want to test\n",
        "    loss_fn                 The loss function we are trying to minimize in training\n",
        "    option                  String to indicate which dataset to use for testing ['val', 'test']\n",
        "    Returns:    Dict containing loss and accuracy of the model on the test dataset\n",
        "    '''\n",
        "    dataloader = loaders[option]\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            temp = model(X)\n",
        "            test_loss += loss_fn(input=temp['out'], target=y).item()\n",
        "            correct += (temp['preds'] == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"{'Validation' if option=='val' else 'Test'} Metrics: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    return {\n",
        "        'loss': test_loss,\n",
        "        'acc': 100*correct\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "o4RrHhmit7tQ"
      },
      "outputs": [],
      "source": [
        "def train_loop_lite(loaders, model, loss_fn, optimizer, interval=75):\n",
        "    '''\n",
        "    Function to train the model and log required information. This is just a lighter version without all the loss logging.\n",
        "    Arguments:\n",
        "    loaders                 dict containing DataLoader objects for the data\n",
        "    model                   The neural network we want to train\n",
        "    loss_fn                 The loss function we are trying to minimize\n",
        "    optimizer               Optimizer that we will use\n",
        "    interval                Interval between logging of loss & calculating test metrics [default: 75]\n",
        "    Returns:    None\n",
        "    '''\n",
        "    dataloader = loaders['train']\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        temp = model(X)\n",
        "        loss = loss_fn(input=temp['out'], target=y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % interval == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            test_loop_lite(loaders, model, loss_fn, option='val')\n",
        "\n",
        "\n",
        "def test_loop_lite(loaders, model, loss_fn, option='val'):\n",
        "    '''\n",
        "    Function to calculate loss and accuracy of the model on the test set. This is just a lighter version without all the loss logging.\n",
        "    Arguments:\n",
        "    loaders                 dict containing DataLoader objects for the data\n",
        "    model                   The neural network we want to test\n",
        "    loss_fn                 The loss function we are trying to minimize in training\n",
        "    option                  String to indicate which dataset to use for testing ['val', 'test']\n",
        "    Returns:    None\n",
        "    '''\n",
        "    dataloader = loaders[option]\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            temp = model(X)\n",
        "            test_loss += loss_fn(input=temp['out'], target=y).item()\n",
        "            correct += (temp['preds'] == y).type(torch.float).sum().item()\n",
        "\n",
        "    correct /= size\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test Metrics: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "a39B0FCJrscM"
      },
      "outputs": [],
      "source": [
        "def plotGraphs(losses, losses_test, acc_test, interval, prefix='', saving=True):\n",
        "    '''\n",
        "    Function to do all the relevant plotting of losses vs iterations\n",
        "    Arguments:\n",
        "    losses          List containing loss on the training set every iteration\n",
        "    losses_test     List containing loss on the test set every 'interval' iterations\n",
        "    acc_test        Accuracy of prediction on the test set every 'interval' iterations\n",
        "    interval        Number of iterations between test set evaluations during training\n",
        "    prefix          Prefix to be added to plot names when saving     (default: '')\n",
        "    saving          Boolean to decide whether we want to save the plots     (default: True)\n",
        "    '''\n",
        "    # Number of iterations carried out during training\n",
        "    num_iters = len(losses)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(num_iters), losses)\n",
        "    plt.grid()\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train Loss Plot')\n",
        "    if saving:\n",
        "        plt.savefig(prefix + '_train_loss.png',bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(list(np.arange(1, num_iters-1, interval)) + [num_iters], losses_test)\n",
        "    plt.grid()\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss Plot')\n",
        "    if saving:\n",
        "        plt.savefig(prefix + '_test_loss.png',bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(list(np.arange(1, num_iters-1, interval)) + [num_iters], acc_test)\n",
        "    plt.grid()\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy Plot')\n",
        "    if saving:\n",
        "        plt.savefig(prefix + '_test_acc.png',bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yams-4tzfoWy"
      },
      "source": [
        "# MNIST using RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Bnzy3HfyMR"
      },
      "source": [
        "## Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "Qk0kfkbzf-XI"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, n_neurons=128, num_layers=1, n_steps=28, n_inputs=28, n_outputs=10):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "\n",
        "        self.n_neurons = n_neurons\n",
        "        self.num_layers = num_layers\n",
        "        self.n_steps = n_steps\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=self.n_inputs,\n",
        "            hidden_size=self.n_neurons,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        states, _ = self.rnn(X)\n",
        "        fcout = self.FC(states[:,-1,:])\n",
        "        out = nn.Softmax(dim=1)(fcout)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "\n",
        "        return {\n",
        "\t\t\t'in': X,\n",
        "\t\t\t'out': out,\n",
        "\t\t\t'preds': preds,\n",
        "            'fcout': fcout,\n",
        "\t\t\t'states':states\n",
        "\t\t}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid Search of Parameters\n",
        "layers_list = [1,2]\n",
        "neurons_list = [32,64]\n",
        "metricsD = {}\n",
        "for layer in layers_list:\n",
        "    metricsD[layer] = {}\n",
        "    for neuron in neurons_list:\n",
        "        metricsD[layer][neuron] = {}\n",
        "\n",
        "# Dictionary of models\n",
        "modelsD = {}\n",
        "for layer in layers_list:\n",
        "    modelsD[layer] = {}\n",
        "    for neuron in neurons_list:\n",
        "        model = VanillaRNN(n_neurons=neuron, num_layers=layer)\n",
        "        modelsD[layer][neuron] = model\n",
        "\n",
        "# RNN1 = VanillaRNN(n_neurons=128, num_layers=1)\n",
        "# print(RNN1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "rjR1heqIf-PZ"
      },
      "outputs": [],
      "source": [
        "# Cross Entropy Loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Number of Epochs\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "\n",
            "CONFIGURATION: num_layers = 1, hidden_size = 32\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Loss: 2.302307  [    0/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 11.6%, Avg loss: 2.302351 \n",
            "\n",
            "Loss: 2.283096  [ 5000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 20.8%, Avg loss: 2.281933 \n",
            "\n",
            "Loss: 2.169864  [10000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 33.4%, Avg loss: 2.171128 \n",
            "\n",
            "Loss: 2.096137  [15000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 37.6%, Avg loss: 2.110651 \n",
            "\n",
            "Loss: 2.128790  [20000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 38.0%, Avg loss: 2.087060 \n",
            "\n",
            "Loss: 2.080314  [25000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 39.4%, Avg loss: 2.070442 \n",
            "\n",
            "Loss: 2.001274  [30000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 44.0%, Avg loss: 2.042150 \n",
            "\n",
            "Loss: 1.995963  [35000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 46.3%, Avg loss: 2.024042 \n",
            "\n",
            "Loss: 2.010761  [40000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 42.5%, Avg loss: 2.056270 \n",
            "\n",
            "Loss: 2.056288  [45000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 49.8%, Avg loss: 1.998942 \n",
            "\n",
            "\n",
            "-----------------------------------------\n",
            "Performance on the Validation set...\n",
            "Validation Metrics: \n",
            "Accuracy: 48.7%, Avg loss: 1.996138 \n",
            "\n",
            "Performance on the Test set...\n",
            "Test Metrics: \n",
            "Accuracy: 48.9%, Avg loss: 1.998512 \n",
            "\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "Time taken for the training: 27.08452 seconds\n",
            "-----------------------------------------\n",
            "\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "\n",
            "CONFIGURATION: num_layers = 1, hidden_size = 64\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Loss: 2.301923  [    0/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 10.0%, Avg loss: 2.301673 \n",
            "\n",
            "Loss: 2.171249  [ 5000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 28.0%, Avg loss: 2.187605 \n",
            "\n",
            "Loss: 2.102137  [10000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 42.0%, Avg loss: 2.098182 \n",
            "\n",
            "Loss: 2.045072  [15000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 46.0%, Avg loss: 2.029741 \n",
            "\n",
            "Loss: 2.045848  [20000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 46.8%, Avg loss: 2.004227 \n",
            "\n",
            "Loss: 1.975366  [25000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 51.9%, Avg loss: 1.958106 \n",
            "\n",
            "Loss: 1.949888  [30000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 52.5%, Avg loss: 1.949870 \n",
            "\n",
            "Loss: 1.941258  [35000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 55.6%, Avg loss: 1.923693 \n",
            "\n",
            "Loss: 1.864862  [40000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 58.5%, Avg loss: 1.903364 \n",
            "\n",
            "Loss: 1.917192  [45000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 62.1%, Avg loss: 1.862562 \n",
            "\n",
            "\n",
            "-----------------------------------------\n",
            "Performance on the Validation set...\n",
            "Validation Metrics: \n",
            "Accuracy: 60.4%, Avg loss: 1.873112 \n",
            "\n",
            "Performance on the Test set...\n",
            "Test Metrics: \n",
            "Accuracy: 60.6%, Avg loss: 1.871473 \n",
            "\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "Time taken for the training: 25.38482 seconds\n",
            "-----------------------------------------\n",
            "\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "\n",
            "CONFIGURATION: num_layers = 2, hidden_size = 32\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Loss: 2.303638  [    0/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 11.4%, Avg loss: 2.302379 \n",
            "\n",
            "Loss: 2.232584  [ 5000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 33.2%, Avg loss: 2.214407 \n",
            "\n",
            "Loss: 2.116945  [10000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 41.1%, Avg loss: 2.110440 \n",
            "\n",
            "Loss: 2.055461  [15000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 48.4%, Avg loss: 2.021921 \n",
            "\n",
            "Loss: 1.973722  [20000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 52.4%, Avg loss: 1.961449 \n",
            "\n",
            "Loss: 1.931006  [25000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 54.7%, Avg loss: 1.926160 \n",
            "\n",
            "Loss: 1.876585  [30000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 63.3%, Avg loss: 1.885336 \n",
            "\n",
            "Loss: 1.894455  [35000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 64.2%, Avg loss: 1.857656 \n",
            "\n",
            "Loss: 1.874784  [40000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 66.3%, Avg loss: 1.834471 \n",
            "\n",
            "Loss: 1.784534  [45000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 68.1%, Avg loss: 1.812897 \n",
            "\n",
            "\n",
            "-----------------------------------------\n",
            "Performance on the Validation set...\n",
            "Validation Metrics: \n",
            "Accuracy: 68.9%, Avg loss: 1.799456 \n",
            "\n",
            "Performance on the Test set...\n",
            "Test Metrics: \n",
            "Accuracy: 68.4%, Avg loss: 1.803612 \n",
            "\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "Time taken for the training: 27.36728 seconds\n",
            "-----------------------------------------\n",
            "\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "\n",
            "CONFIGURATION: num_layers = 2, hidden_size = 64\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Loss: 2.302832  [    0/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 15.3%, Avg loss: 2.302786 \n",
            "\n",
            "Loss: 2.123043  [ 5000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 33.7%, Avg loss: 2.147471 \n",
            "\n",
            "Loss: 2.011013  [10000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 52.1%, Avg loss: 1.993655 \n",
            "\n",
            "Loss: 1.940380  [15000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 59.1%, Avg loss: 1.909236 \n",
            "\n",
            "Loss: 1.838384  [20000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 61.5%, Avg loss: 1.873081 \n",
            "\n",
            "Loss: 1.891853  [25000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 63.6%, Avg loss: 1.835100 \n",
            "\n",
            "Loss: 1.828219  [30000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 63.6%, Avg loss: 1.844030 \n",
            "\n",
            "Loss: 1.754793  [35000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 65.9%, Avg loss: 1.806712 \n",
            "\n",
            "Loss: 1.898403  [40000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 68.6%, Avg loss: 1.787090 \n",
            "\n",
            "Loss: 1.830779  [45000/50000]\n",
            "Validation Metrics: \n",
            "Accuracy: 69.1%, Avg loss: 1.787068 \n",
            "\n",
            "\n",
            "-----------------------------------------\n",
            "Performance on the Validation set...\n",
            "Validation Metrics: \n",
            "Accuracy: 72.8%, Avg loss: 1.745817 \n",
            "\n",
            "Performance on the Test set...\n",
            "Test Metrics: \n",
            "Accuracy: 72.3%, Avg loss: 1.749662 \n",
            "\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "Time taken for the training: 31.43112 seconds\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        model = modelsD[layer][neuron]\n",
        "        print('\\n&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\\n')\n",
        "        print(f'CONFIGURATION: num_layers = {layer}, hidden_size = {neuron}')\n",
        "\n",
        "        # Adam optimizer\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Measuring time taken for the training process\n",
        "        losses = []\n",
        "        losses_test = []\n",
        "        acc_test = []\n",
        "        interval = 50\n",
        "        start = time.time()\n",
        "        for i in range(num_epochs):\n",
        "            print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "            \n",
        "            # Training the network for this epoch\n",
        "            temp = train_loop(loaders, model, loss_fn, optimizer, interval)\n",
        "            losses += temp['losses']\n",
        "            losses_test += temp['losses_test']\n",
        "            acc_test += temp['acc_test']\n",
        "\n",
        "        # Testing on the validation set\n",
        "        print('\\n-----------------------------------------')\n",
        "        print('Performance on the Validation set...')\n",
        "        temp = test_loop(loaders, model, loss_fn, option='val')\n",
        "        losses_test.append(temp['loss'])\n",
        "        acc_test.append(temp['acc'])\n",
        "\n",
        "        # Testing on the test set\n",
        "        print('Performance on the Test set...')\n",
        "        temp = test_loop(loaders, model, loss_fn, option='test')\n",
        "\n",
        "        # Logging the losses and accuracies obtained\n",
        "        metricsD[layer][neuron]['l'] = losses\n",
        "        metricsD[layer][neuron]['lt'] = losses_test\n",
        "        metricsD[layer][neuron]['at'] = acc_test\n",
        "\n",
        "        end = time.time()\n",
        "        print('-----------------------------------------')\n",
        "        print('-----------------------------------------')\n",
        "        print('Time taken for the training: {0:.5f} seconds'.format(end-start))\n",
        "        print('-----------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the relevant graphs and plots\n",
        "saving = False\n",
        "dataloader = loaders['test']\n",
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        model = modelsD[layer][neuron]\n",
        "        prefix = direc_pics + 'mnist_RNN_l{}_n{}'.format(layer, neuron)\n",
        "        plotGraphs(\n",
        "            losses = metricsD[layer][neuron]['l'],\n",
        "            losses_test = metricsD[layer][neuron]['lt'],\n",
        "            acc_test = metricsD[layer][neuron]['at'],\n",
        "            interval = interval,\n",
        "            prefix = prefix,\n",
        "            saving = saving\n",
        "        )\n",
        "\n",
        "        # Plotting few pictures with true and predicted labels\n",
        "        with torch.no_grad():\n",
        "            for X, y in dataloader:\n",
        "                temp = model(X)\n",
        "            \n",
        "            figure = plt.figure(figsize=(10, 10))\n",
        "            cols, rows = 5, 5\n",
        "            for i in range(1, cols * rows + 1):\n",
        "                sample_idx = torch.randint(5000, size=(1,)).item()\n",
        "                img, label = X[sample_idx], y[sample_idx]\n",
        "                figure.add_subplot(rows, cols, i)\n",
        "                plt.title('Truth: {} | Pred: {}'.format(label, temp['preds'][sample_idx]), fontsize=9)\n",
        "                plt.axis(\"off\")\n",
        "                plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "            if saving:\n",
        "                plt.savefig(prefix + '_samples.png',bbox_inches='tight')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the trained network\n",
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        torch.save(modelsD[layer][neuron], direc_main+'mnist_RNN_l{}_n{}.pth'.format(layer, neuron))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The exact same above has to be done with regularization on a new set of models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nXYCW_Gf9UY"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjGK-vh6gBCA"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, n_neurons=128, num_layers=1, n_steps=28, n_inputs=28, n_outputs=10):\n",
        "        super(GRU, self).__init__()\n",
        "        \n",
        "        self.n_neurons = n_neurons\n",
        "        self.num_layers = num_layers\n",
        "        self.n_steps = n_steps\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.n_inputs,\n",
        "            hidden_size=self.n_neurons,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        states, _ = self.gru(X)\n",
        "        fcout = self.FC(states[:,-1,:])\n",
        "        out = nn.Softmax(dim=1)(fcout)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "\n",
        "        return {\n",
        "\t\t\t'in': X,\n",
        "\t\t\t'out': out,\n",
        "\t\t\t'preds': preds,\n",
        "            'fcout': fcout,\n",
        "\t\t\t'states':states\n",
        "\t\t}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid Search of Parameters\n",
        "layers_list = [1,2]\n",
        "neurons_list = [32,64]\n",
        "metricsD = {}\n",
        "for layer in layers_list:\n",
        "    metricsD[layer] = {}\n",
        "    for neuron in neurons_list:\n",
        "        metricsD[layer][neuron] = {}\n",
        "\n",
        "# Dictionary of models\n",
        "modelsD = {}\n",
        "for layer in layers_list:\n",
        "    modelsD[layer] = {}\n",
        "    for neuron in neurons_list:\n",
        "        model = GRU(n_neurons=neuron, num_layers=layer)\n",
        "        modelsD[layer][neuron] = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross Entropy Loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Number of Epochs\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        model = modelsD[layer][neuron]\n",
        "        print('\\n&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\\n')\n",
        "        print(f'CONFIGURATION: num_layers = {layer}, hidden_size = {neuron}')\n",
        "\n",
        "        # Adam optimizer\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Measuring time taken for the training process\n",
        "        losses = []\n",
        "        losses_test = []\n",
        "        acc_test = []\n",
        "        interval = 50\n",
        "        start = time.time()\n",
        "        for i in range(num_epochs):\n",
        "            print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "            \n",
        "            # Training the network for this epoch\n",
        "            temp = train_loop(loaders, model, loss_fn, optimizer, interval)\n",
        "            losses += temp['losses']\n",
        "            losses_test += temp['losses_test']\n",
        "            acc_test += temp['acc_test']\n",
        "\n",
        "        # Testing on the validation set\n",
        "        print('\\n-----------------------------------------')\n",
        "        print('Performance on the Validation set...')\n",
        "        temp = test_loop(loaders, model, loss_fn, option='val')\n",
        "        losses_test.append(temp['loss'])\n",
        "        acc_test.append(temp['acc'])\n",
        "\n",
        "        # Testing on the test set\n",
        "        print('Performance on the Test set...')\n",
        "        temp = test_loop(loaders, model, loss_fn, option='test')\n",
        "\n",
        "        # Logging the losses and accuracies obtained\n",
        "        metricsD[layer][neuron]['l'] = losses\n",
        "        metricsD[layer][neuron]['lt'] = losses_test\n",
        "        metricsD[layer][neuron]['at'] = acc_test\n",
        "\n",
        "        end = time.time()\n",
        "        print('-----------------------------------------')\n",
        "        print('-----------------------------------------')\n",
        "        print('Time taken for the training: {0:.5f} seconds'.format(end-start))\n",
        "        print('-----------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the relevant graphs and plots\n",
        "saving = False\n",
        "dataloader = loaders['test']\n",
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        model = modelsD[layer][neuron]\n",
        "        prefix = direc_pics + 'mnist_GRU_l{}_n{}'.format(layer, neuron)\n",
        "        plotGraphs(\n",
        "            losses = metricsD[layer][neuron]['l'],\n",
        "            losses_test = metricsD[layer][neuron]['lt'],\n",
        "            acc_test = metricsD[layer][neuron]['at'],\n",
        "            interval = interval,\n",
        "            prefix = prefix,\n",
        "            saving = saving\n",
        "        )\n",
        "\n",
        "        # Plotting few pictures with true and predicted labels\n",
        "        with torch.no_grad():\n",
        "            for X, y in dataloader:\n",
        "                temp = model(X)\n",
        "            \n",
        "            figure = plt.figure(figsize=(10, 10))\n",
        "            cols, rows = 5, 5\n",
        "            for i in range(1, cols * rows + 1):\n",
        "                sample_idx = torch.randint(5000, size=(1,)).item()\n",
        "                img, label = X[sample_idx], y[sample_idx]\n",
        "                figure.add_subplot(rows, cols, i)\n",
        "                plt.title('Truth: {} | Pred: {}'.format(label, temp['preds'][sample_idx]), fontsize=9)\n",
        "                plt.axis(\"off\")\n",
        "                plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "            if saving:\n",
        "                plt.savefig(prefix + '_samples.png',bbox_inches='tight')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the trained network\n",
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        torch.save(modelsD[layer][neuron], direc_main+'mnist_GRU_l{}_n{}.pth'.format(layer, neuron))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The exact same above has to be done with regularization on a new set of models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6dpn-SjgCFC"
      },
      "source": [
        "## Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58YxxuOTgHA9"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, n_neurons=128, num_layers=1, n_steps=28, n_inputs=28, n_outputs=10):\n",
        "        super(GRU, self).__init__()\n",
        "        \n",
        "        self.n_neurons = n_neurons\n",
        "        self.num_layers = num_layers\n",
        "        self.n_steps = n_steps\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.n_inputs,\n",
        "            hidden_size=self.n_neurons,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        states, _ = self.gru(X)\n",
        "        fcout = self.FC(states[:,-1,:])\n",
        "        out = nn.Softmax(dim=1)(fcout)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "\n",
        "        return {\n",
        "\t\t\t'in': X,\n",
        "\t\t\t'out': out,\n",
        "\t\t\t'preds': preds,\n",
        "            'fcout': fcout,\n",
        "\t\t\t'states':states\n",
        "\t\t}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9eA6SMmgHoj"
      },
      "outputs": [],
      "source": [
        "# Grid Search of Parameters\n",
        "layers_list = [1,2]\n",
        "neurons_list = [32,64]\n",
        "metricsD = {}\n",
        "for layer in layers_list:\n",
        "    metricsD[layer] = {}\n",
        "    for neuron in neurons_list:\n",
        "        metricsD[layer][neuron] = {}\n",
        "\n",
        "# Dictionary of models\n",
        "modelsD = {}\n",
        "for layer in layers_list:\n",
        "    modelsD[layer] = {}\n",
        "    for neuron in neurons_list:\n",
        "        model = GRU(n_neurons=neuron, num_layers=layer)\n",
        "        modelsD[layer][neuron] = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross Entropy Loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Number of Epochs\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        model = modelsD[layer][neuron]\n",
        "        print('\\n&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\\n')\n",
        "        print(f'CONFIGURATION: num_layers = {layer}, hidden_size = {neuron}')\n",
        "\n",
        "        # Adam optimizer\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Measuring time taken for the training process\n",
        "        losses = []\n",
        "        losses_test = []\n",
        "        acc_test = []\n",
        "        interval = 50\n",
        "        start = time.time()\n",
        "        for i in range(num_epochs):\n",
        "            print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "            \n",
        "            # Training the network for this epoch\n",
        "            temp = train_loop(loaders, model, loss_fn, optimizer, interval)\n",
        "            losses += temp['losses']\n",
        "            losses_test += temp['losses_test']\n",
        "            acc_test += temp['acc_test']\n",
        "\n",
        "        # Testing on the validation set\n",
        "        print('\\n-----------------------------------------')\n",
        "        print('Performance on the Validation set...')\n",
        "        temp = test_loop(loaders, model, loss_fn, option='val')\n",
        "        losses_test.append(temp['loss'])\n",
        "        acc_test.append(temp['acc'])\n",
        "\n",
        "        # Testing on the test set\n",
        "        print('Performance on the Test set...')\n",
        "        temp = test_loop(loaders, model, loss_fn, option='test')\n",
        "\n",
        "        # Logging the losses and accuracies obtained\n",
        "        metricsD[layer][neuron]['l'] = losses\n",
        "        metricsD[layer][neuron]['lt'] = losses_test\n",
        "        metricsD[layer][neuron]['at'] = acc_test\n",
        "\n",
        "        end = time.time()\n",
        "        print('-----------------------------------------')\n",
        "        print('-----------------------------------------')\n",
        "        print('Time taken for the training: {0:.5f} seconds'.format(end-start))\n",
        "        print('-----------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the relevant graphs and plots\n",
        "saving = False\n",
        "dataloader = loaders['test']\n",
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        model = modelsD[layer][neuron]\n",
        "        prefix = direc_pics + 'mnist_GRU_l{}_n{}'.format(layer, neuron)\n",
        "        plotGraphs(\n",
        "            losses = metricsD[layer][neuron]['l'],\n",
        "            losses_test = metricsD[layer][neuron]['lt'],\n",
        "            acc_test = metricsD[layer][neuron]['at'],\n",
        "            interval = interval,\n",
        "            prefix = prefix,\n",
        "            saving = saving\n",
        "        )\n",
        "\n",
        "        # Plotting few pictures with true and predicted labels\n",
        "        with torch.no_grad():\n",
        "            for X, y in dataloader:\n",
        "                temp = model(X)\n",
        "            \n",
        "            figure = plt.figure(figsize=(10, 10))\n",
        "            cols, rows = 5, 5\n",
        "            for i in range(1, cols * rows + 1):\n",
        "                sample_idx = torch.randint(5000, size=(1,)).item()\n",
        "                img, label = X[sample_idx], y[sample_idx]\n",
        "                figure.add_subplot(rows, cols, i)\n",
        "                plt.title('Truth: {} | Pred: {}'.format(label, temp['preds'][sample_idx]), fontsize=9)\n",
        "                plt.axis(\"off\")\n",
        "                plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "            if saving:\n",
        "                plt.savefig(prefix + '_samples.png',bbox_inches='tight')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the trained network\n",
        "for layer in modelsD:\n",
        "    for neuron in modelsD[layer]:\n",
        "        torch.save(modelsD[layer][neuron], direc_main+'mnist_GRU_l{}_n{}.pth'.format(layer, neuron))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The exact same above has to be done with regularization on a new set of models\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EE6132_Lab4_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
